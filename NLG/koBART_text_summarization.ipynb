{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"koBART_text_summarization_git_upload.ipynb","provenance":[{"file_id":"1RYTiBsRnfXusjFVZbdfGtfm154_Bdz4J","timestamp":1647423013496}],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyMGRWbFIOgpyfPFWe+YnV8F"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# 환경 세팅"],"metadata":{"id":"9Z-LqrFTi6Lj"}},{"cell_type":"code","source":["!pip install transformers pytorch_lightning pickle5"],"metadata":{"id":"GL87QgBOi9oz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pickle5 as pickle\n","import numpy as np\n","import pandas as pd\n","import os\n","from sklearn.model_selection import train_test_split\n","\n","import torch\n","from torch.optim import AdamW\n","from torch.utils.data import Dataset, DataLoader\n","\n","import pytorch_lightning as pl\n","from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor\n","from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n","from pytorch_lightning.loggers import TensorBoardLogger\n","from pytorch_lightning import loggers as pl_loggers\n","\n","from transformers import PreTrainedTokenizerFast\n","from transformers import BartForConditionalGeneration\n","from transformers.optimization import get_cosine_schedule_with_warmup\n","\n","import logging\n","\n","logger = logging.getLogger()\n","logger.setLevel(logging.INFO)"],"metadata":{"id":"NYRMmwzISCnM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"wutMC0mxU8IO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cd "],"metadata":{"id":"EG4ORIK2R8fQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Make input data"],"metadata":{"id":"ItWvyPURi_Ap"}},{"cell_type":"code","source":["with open('./data/data.pickle', 'rb') as f:\n","    data = pickle.load(f)"],"metadata":{"id":"apoRafZtU8Kr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = pd.DataFrame(data, columns =['text', 'summary'])\n","df = df.drop_duplicates(subset=None, keep='first', inplace=False, ignore_index=False)\n","df = df.sample(frac=1, random_state=42).reset_index(drop=True) \n","\n","# train, validation, test 분리\n","doc_len = df.shape[0]\n","\n","train = df[:7000]\n","val = df[7000:8000]\n","test = df[8000:]\n"],"metadata":{"id":"bEfvIo_JtTa0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# aihub 데이터 불러오기\n","with open('./data/aihub.pickle', 'rb') as f:\n","    data_ai = pickle.load(f)\n","  \n","ai_df = pd.DataFrame(data_ai, columns =['text', 'summary'])\n","\n","train = pd.concat([train, ai_df])"],"metadata":{"id":"VEnrmXcO4ove"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"total_shape:\", df.shape)\n","print(\"train_shape:\", train.shape)\n","print(\"val_shape:\", val.shape)\n","print(\"test_shape:\", test.shape)"],"metadata":{"id":"0E6sd13Vsm8O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train.head()"],"metadata":{"id":"i_IwKs9fMzaR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class SummaryDataset(Dataset):\n","    def __init__(self, \n","                 data: pd.DataFrame,\n","                 tokenizer: PreTrainedTokenizerFast,\n","                 max_len:int=512,\n","                 ignore_index=-100):\n","      \n","        super().__init__()\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","        self.data = data\n","        self.len = self.data.shape[0]\n","\n","        self.pad_index = self.tokenizer.pad_token_id\n","        self.ignore_index = ignore_index\n","\n","    def add_padding_data(self, inputs):\n","        if len(inputs) < self.max_len:\n","            pad = np.array([self.pad_index] *(self.max_len - len(inputs)))\n","            inputs = np.concatenate([inputs, pad])\n","        else:\n","            inputs = inputs[:self.max_len]\n","\n","        return inputs\n","\n","    def add_ignored_data(self, inputs):\n","        if len(inputs) < self.max_len:\n","            pad = np.array([self.ignore_index] *(self.max_len - len(inputs)))\n","            inputs = np.concatenate([inputs, pad])\n","        else:\n","            inputs = inputs[:self.max_len]\n","\n","        return inputs\n","    \n","    def __getitem__(self, idx):\n","        instance = self.data.iloc[idx]\n","        input_ids = self.tokenizer.encode(instance['text'])\n","        input_ids = self.add_padding_data(input_ids)\n","\n","        label_ids = self.tokenizer.encode(instance['summary'])\n","        label_ids.append(self.tokenizer.eos_token_id)\n","        decoder_input_ids = [self.tokenizer.eos_token_id]\n","        decoder_input_ids += label_ids[:-1]\n","        decoder_input_ids = self.add_padding_data(decoder_input_ids)\n","        label_ids = self.add_ignored_data(label_ids)\n","\n","        result = {'input_ids': np.array(input_ids, dtype=np.int_),\n","                  'decoder_input_ids': np.array(decoder_input_ids, dtype=np.int_),\n","                  'labels': np.array(label_ids, dtype=np.int_),\n","                  }\n","        return result\n","\n","    def __len__(self):\n","        return self.len\n"],"metadata":{"id":"XO_o0KyRxNh-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class SummaryDataModule(pl.LightningDataModule):\n","    def __init__(self, \n","                 train_df: pd.DataFrame,\n","                 val_df: pd.DataFrame,\n","                 test_df: pd.DataFrame, \n","                 tokenizer: PreTrainedTokenizerFast,\n","                 max_len:int=512,\n","                 batch_size:int=8,\n","                 num_workers:int=2):\n","      \n","        super().__init__()\n","        self.batch_size = batch_size\n","        self.max_len = max_len\n","        self.train_df = train_df\n","        self.val_df = val_df\n","        self.test_df = test_df\n","        self.tokenizer = tokenizer\n","        self.num_workers = num_workers\n","\n","    def setup(self, stage):\n","        self.train = SummaryDataset(self.train_df,\n","                                    self.tokenizer,\n","                                    self.max_len)\n","        self.val = SummaryDataset(self.val_df,\n","                                   self.tokenizer,\n","                                   self.max_len)\n","        self.test = SummaryDataset(self.test_df,\n","                                   self.tokenizer,\n","                                   self.max_len)\n","\n","    def train_dataloader(self):\n","        train = DataLoader(self.train,\n","                           batch_size=self.batch_size,\n","                           num_workers=self.num_workers, \n","                           shuffle=True)\n","        return train\n","\n","    def val_dataloader(self):\n","        val = DataLoader(self.val,\n","                         batch_size=self.batch_size,\n","                         num_workers=self.num_workers, \n","                         shuffle=False)\n","        return val\n","\n","    def test_dataloader(self):\n","        test = DataLoader(self.test,\n","                          batch_size=self.batch_size,\n","                          num_workers=self.num_workers, \n","                          shuffle=False)\n","        return test"],"metadata":{"id":"-PwhKlBbuCsU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v1')\n","data_module = SummaryDataModule(train, val, test, \n","                                tokenizer,\n","                                batch_size=8,\n","                                max_len=512,\n","                                num_workers=2)"],"metadata":{"id":"DXpcxwljwQrA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Model"],"metadata":{"id":"31E-w1SzjFs_"}},{"cell_type":"code","source":["class KoBARTConditionalGeneration(pl.LightningModule):\n","    def __init__(self):\n","        super().__init__()\n","        self.model = BartForConditionalGeneration.from_pretrained('gogamza/kobart-base-v1')\n","        self.model.train()\n","        self.bos_token = '<s>'\n","        self.eos_token = '</s>'\n","        \n","        self.tokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v1')\n","        self.pad_token_id = self.tokenizer.pad_token_id\n","\n","    def forward(self, inputs):\n","\n","        attention_mask = inputs['input_ids'].ne(self.pad_token_id).float()\n","        decoder_attention_mask = inputs['decoder_input_ids'].ne(self.pad_token_id).float()\n","        \n","        return self.model(input_ids=inputs['input_ids'],\n","                          attention_mask=attention_mask,\n","                          decoder_input_ids=inputs['decoder_input_ids'],\n","                          decoder_attention_mask=decoder_attention_mask,\n","                          labels=inputs['labels'], return_dict=True)\n","        \n","    def setup_steps(self, stage=None):\n","        train_loader = self.trainer._data_connector._train_dataloader_source.dataloader()\n","        return len(train_loader)\n","\n","    def configure_optimizers(self):\n","        lr = 3e-5\n","        num_workers = 2\n","        batch_size = 8\n","        max_epochs = 30\n","        warmup_ratio = 0.1\n","\n","        param_optimizer = list(self.model.named_parameters())\n","        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n","        optimizer_grouped_parameters = [\n","            {'params': [p for n, p in param_optimizer if not any(\n","                nd in n for nd in no_decay)], 'weight_decay': 0.01},\n","            {'params': [p for n, p in param_optimizer if any(\n","                nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","        ]\n","        \n","        optimizer = AdamW(optimizer_grouped_parameters, lr=lr)\n","\n","        data_len = self.setup_steps(self)\n","        logging.info(f'number of workers {num_workers}, data length {data_len}')\n","        num_train_steps = int(data_len / (batch_size * num_workers) * max_epochs)\n","        logging.info(f'num_train_steps : {num_train_steps}')\n","        num_warmup_steps = int(num_train_steps * warmup_ratio)\n","        logging.info(f'num_warmup_steps : {num_warmup_steps}')\n","        scheduler = get_cosine_schedule_with_warmup(optimizer,\n","                                                    num_warmup_steps=num_warmup_steps, \n","                                                    num_training_steps=num_train_steps)\n","        lr_scheduler = {'scheduler': scheduler, \n","                        'monitor': 'loss', 'interval': 'step',\n","                        'frequency': 1}\n","\n","        return [optimizer], [lr_scheduler]\n","\n","\n","    def training_step(self, batch, batch_idx):\n","        outs = self(batch)\n","        loss = outs.loss\n","        self.log('train_loss', loss, prog_bar=True, logger=True, batch_size=len(batch))\n","        return loss\n","\n","    def validation_step(self, batch, batch_idx):\n","        outs = self(batch)\n","        loss = outs['loss']\n","        return (loss)\n","\n","    def validation_epoch_end(self, outputs):\n","        losses = []\n","        for loss in outputs:\n","            losses.append(loss)\n","        self.log('val_loss', torch.stack(losses).mean(), prog_bar=True)"],"metadata":{"id":"DxajokybTr3g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["EPOCH = 30\n","checkpoint_callback = ModelCheckpoint(dirpath=\"checkpoints\",\n","                                      filename=\"bast-checkpoint\",\n","                                      save_top_k=1,\n","                                      verbose=True,\n","                                      monitor=\"val_loss\",\n","                                      mode=\"min\")\n","\n","early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=0.001, patience=3)\n","\n","logger = TensorBoardLogger(\"lightning_logs\", name=\"summary\")\n","\n","trainer = pl.Trainer(logger=logger,\n","                     checkpoint_callback=[checkpoint_callback, early_stop_callback],\n","                     max_epochs=EPOCH,\n","                     gpus=1,\n","                     progress_bar_refresh_rate=30)\n","\n","model = KoBARTConditionalGeneration()"],"metadata":{"id":"DRiOH-BbFjyK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer.fit(model, data_module)"],"metadata":{"id":"L-fwII1wFnjj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%load_ext tensorboard\n","%tensorboard --logdir ./lightning_logs"],"metadata":{"id":"rQhi4Ie6eWbt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trained_model = KoBARTConditionalGeneration.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n","trained_model.freeze()\n","\n","trained_model.model.save_pretrained(\"./model\")"],"metadata":{"id":"hlNV6rJxFnlp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def summarize(text):\n","  _load_model = BartForConditionalGeneration.from_pretrained('./model')\n","  raw_input_ids = tokenizer.encode(text)\n","  input_ids = [tokenizer.bos_token_id] + raw_input_ids + [tokenizer.eos_token_id]\n","  summary_ids = _load_model.generate(torch.tensor([input_ids]),  num_beams=4,  max_length=512,  eos_token_id=1)\n","  preds = tokenizer.decode(summary_ids.squeeze().tolist(), skip_special_tokens=True)\n","\n","  return preds\n","  "],"metadata":{"id":"uKj1zKAcHFsY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sample_row = test.iloc[1]\n","text = sample_row[\"text\"]\n","model_summary = summarize(text)"],"metadata":{"id":"7UJNfJfqHFu8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(text)"],"metadata":{"id":"ijpvQkTQDNfJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(sample_row[\"summary\"])"],"metadata":{"id":"heSrP4TeFmv2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(model_summary)"],"metadata":{"id":"9AIGduDiFm22"},"execution_count":null,"outputs":[]}]}